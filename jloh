#!/usr/bin/env python3

# version and info
script_info = """
###
Author: Matteo Schiavinato
Based on the work of: Leszek Pryszcz (2014) and Veronica Mixao (2018)
Last change: 15/12/2021
###
"""

# import modules
import argparse as ap
import os
import sys
import pysam
import subprocess
from time import asctime as at
import pybedtools
from pybedtools import BedTool

# help section
if len(sys.argv) == 1:
	sys.argv.append("--help")

if (sys.argv[1] in ["--help", "-h", "-help", "help", "getopt", "usage"]):
	sys.stderr.write("""

J LOH
Still the one from the block
-
Extact LOH blocks from SNPs and a reference genome
-
v 0.4.0
--------------------------------------------------------------------------------
by Matteo Schiavinato
largely based on:
Pryszcz et al., 2014	https://doi.org/10.1093/gbe/evu082
Mixao et al., 2019		https://doi.org/10.3389/fgene.2019.00383
--------------------------------------------------------------------------------

Usage:
./jloh --vcf <VCF> --genome-file <GENOME_FILE> --bam <BAM> [options]

[mandatory]
--vcf               Input VCF file containing all types of variants             [!]
--genome-file       File with chromosome lengths (chromosome <TAB> size)        [!]
--bam               BAM file used to call the --vcf variants                    [!]

[optional]
--sample            Sample name / Strain name for output files                  [loh_blocks]
--output-dir        Output directory                                            [loh_blocks_out]
--t0-vcf            VCF with variants to ignore from --vcf                      [off]
--t0-bam            BAM file used to call the --t0-vcf variants                 [off]
--t0-filter-type    What to do with t0 LOH events? "keep" or "remove"           [remove]
--min-het-snps      Min. num. of heterozygous SNPs in heterozygous region       [2]
--snp-distance      Max. distance (bp between SNPs for blocks definition        [100]
--block-dist        Combine LOH blocks into one if closer than this distance    [100]
--min-size          Min. LOH block size                                         [100]
--min-af            Min. allele frequency to consider a variant heterozygous    [0.3]
--max-af            Max. allele frequency to consider a variant heterozygous    [0.7]
--min-frac-cov      Min. mean coverage fraction for LOH blocks                  [0.75]
                    (used only if --bam specified)
--max-frac-cov      Max. mean coverage fraction for LOH blocks                  [1.25]
                    (used only if --bam specified)
--print-info        Show authors and edits with dates                           [off]

""")
	sys.exit(0)

# argument parser
p = ap.ArgumentParser(description="")
p.add_argument("--vcf", type=str)
p.add_argument("--genome-file")
p.add_argument("--sample", default="loh_blocks")
p.add_argument("--output-dir", default="loh_blocks_out")
p.add_argument("--t0-vcf", type=str, default=None)
p.add_argument("--t0-bam", type=str, default=None)
p.add_argument("--t0-filter-type", choices=["keep", "remove"], default="remove")
p.add_argument("--min-het-snps", type=float, default=2)
p.add_argument("--snp-distance", default=100, type=int)
p.add_argument("--block-dist", default=100, type=int)
p.add_argument("--min-size", default=100, type=int)
p.add_argument("--min-af", default=0.3, type=float)
p.add_argument("--max-af", default=0.7, type=float)
p.add_argument("--bam", type=str)
p.add_argument("--min-frac-cov", type=float, default=0.75)
p.add_argument("--max-frac-cov", type=float, default=1.25)
p.add_argument("--print-info", action="store_true")
args = p.parse_args()


# functions
def organize_workspace(output_dir, min_frac_cov, max_frac_cov, bam):

	"""
	Creation of folders and workspace where the script has to operate
	Verify presence of minimal set of files to work with
	"""

	if os.path.exists(output_dir) == False:
		os.makedirs(output_dir)
		sys.stderr.write(f"[{at()}] Created output directory {output_dir}\n")

	if os.path.exists(f"{output_dir}/process") == False:
		os.mkdir(f"{output_dir}/process")

	# check if bam is used or not
	if bam and not (min_frac_cov or max_frac_cov):
		sys.stderr.write(f"[{at()}] ERROR: If you specify a --bam file you may want to use --min-frac-cov or --max-frac-cov\n")
		sys.exit()

	if (min_frac_cov or max_frac_cov) and not bam:
		sys.stderr.write(f"[{at()}] ERROR: If you use --min-frac-cov or --max-frac-cov you may want to specify a --bam file\n")
		sys.exit()

	return (True, output_dir)



def extract_heterozygous_snps(vcf, min_af, max_af, output_dir, sample, het_snps_vcf):

	"""
	v2
	Author: Matteo Schiavinato
	Last change: 14/12/2021
	Note: Read the VCF file containing SNPs and indels
	Output only heterozygous SNPs, including number of kept and removed lines
	"""

	def extract_header_line(line):
		if line[0] == "#":
			return line
		else:
			return False

	def extract_heterozygous_line(line):

		if line[0] != "#":

			# split by field
			lst = line.rstrip("\b\r\n").split("\t")

			# read values
			annotations = lst[8].split(":")
			values = lst[9].split(":")
			dict = { annotations[i]:values[i] for i in range(0, len(annotations)) }

			# if it's a single heterozygous SNP
			if ((len(lst[3]) == len(lst[4]) == 1) and (dict["GT"]=="0/1")):

				# does AF fit the criteria?
				if (min_af <= float(dict["AF"]) <= max_af):

					# 4. write out lines that have fitting values
					return line

				# remove those without the frequency in range
				else:
					return False

			# consider multiallelic sites
			elif ((len(lst[3]) == 1) and (len(lst[4].split(",")) > 1)):

				# this means that there are two annotations for AF
				# and that both have to be checked
				# so it is a variation on the previous block
				# which could be rendered into a function
				# assuming that there are > 1 AF annotation
				# splitting the field based on the comma
				# and converting to float the content
				AFs = [ float(x) for x in dict["AF"].split(",") ]

				# all it takes is one of the variants to be heterozygous
				# for the locus to be conserved
				# where het = AF comprised between --min-af and --max-af
				Conditions = [ float(args.min_af) <= af <= float(args.max_af) for af in AFs]
				if any(Conditions) == True:

					# 4. write out lines that have fitting values
					return line

				# remove those without any heterozygous snp
				# among those available
				else:
					return False

			# remove homozygous SNPs
			# and indels
			else:
				return False
		# exclude header lines starting with "#"
		else:
			return False

	# read header
	INPUT = open(vcf, "r")
	New_lines_header = [ extract_header_line(line) for line in INPUT ]
	New_lines_header = [ i for i in New_lines_header if i != False ]
	INPUT.close()

	# read heterozygous lines
	INPUT = open(vcf, "r")
	New_lines_vcf = [ extract_heterozygous_line(line) for line in INPUT ]
	removed = len([i for i in New_lines_vcf if i == False])
	New_lines_vcf = [ i for i in New_lines_vcf if i != False ]
	kept = len(New_lines_vcf)
	INPUT.close()

	# merge
	New_lines = New_lines_header + New_lines_vcf

	sys.stderr.write(f"[{at()}] Kept {kept} heterozygous SNPs ({round((kept / (kept + removed))*100, 2)} %) out of {kept+removed} VCF records\n")

	# write filtered bed file lines to an actual bed file output
	OUT = open(het_snps_vcf, "w")
	for line in New_lines:
		OUT.write(line)
	OUT.close()

	return (True, het_snps_vcf)


def count_snps_in_intervals(sample, output_dir, het_snps_vcf, bed_out, snp_distance):

	"""
	v1
	Create BED regions based on the input VCF rows after their filtering
	These regions merge together SNPs as long as they are within --snp-distance
	For each, the number of variants contained in it are counted
	The output is: chr | start | end | count
	Last update: 15/12/2021
	"""

	bt = BedTool(het_snps_vcf)
	bt_merged = bt.merge(d=snp_distance, c=1, o="count")
	bt_merged.saveas(bed_out)

	return True


def filter_by_snp_density(in_bed, out_bed, min_num_snps):

	"""
	Filter regions based on the number of variants found in them
	Which were counted in the previous function

	v1
	Author: Leszek Pryszcz
	Notes: Parse BED and print stats
	Last change: 14/02/2014
	###
	v2
	Author: Matteo Schiavinato
	Notes: included a return statement to fit the new code, update to python3
	Last change: 15/12/2021
	"""

	handle = open(in_bed, "r")

	out = []
	skipped = 0
	for l in handle:
		ref,start,end,count = l.split()
		ref,start,end,count = str(ref),int(start),int(end),float(count)
		if count >= min_num_snps:
			out.append(l)
		else:
			skipped += 1

	# write filtered bed file lines to an actual bed file output
	OUT = open(out_bed, "w")
	for line in out:
		OUT.write(line)
	OUT.close()

	return (True, skipped)


def get_complementary_regions(in_bed, out_bed, genome_file):

	"""
	Run bedtools complement to obtain BED intervals
	These intervals represent the complement of the original regions
	Which in turn represented the heterozygous blocks
	Hence, the output of this function are the homozygous blocks
	"""

	bt = BedTool(in_bed)
	bt_comp = bt.complement(g=genome_file)
	bt_comp.saveas(out_bed)

	return True


def merge_bed_homo_intervals(sample, output_dir, in_bed, out_bed, block_dist):

	"""
	v1
	Author: Matteo Schiavinato
	Note: Combine neighboring homoyzgous blocks if their distance < block_dist
	Last update: 15/12/2021
	"""

	bt = BedTool(in_bed)
	bt_merged = bt.merge(d=block_dist)
	bt_merged.saveas(out_bed)

	return True


def filter_by_length(in_bed, out_bed, min_size):

	"""
	Keep only candidate LOH regions of a certain minimum size
	"""

	INPUT = open(in_bed, "r")
	OUTPUT = open(out_bed, "w")
	removed = 0

	for line in INPUT:
		chr,start,end = line.rstrip("\n\r\b").split("\t")
		length = int(end) - int(start)

		if length >= min_size:
			out_lst = [str(i) for i in [chr,start,end,length]]
			OUTPUT.write("\t".join(out_lst) + "\n")

		else:
			removed += 1

	INPUT.close()
	OUTPUT.close()

	return (True, removed)


def filter_by_coverage(input_bam, bed_file, covfr, cmax, output_dir, sample, handle):

	"""
	v1
	Author: Leszek Pryszcz
	Notes: Report only windows with at least mincov of mean
	Last change: 14/02/2014
	###
	v2
	Author: Matteo Schiavinato
	Notes: update to python3
	Last change: 26/11/2021
	###
	v3
	Author: Matteo Schiavinato
	Notes: Changed how global coverage is calculated; syntax adjustments
	Last change: 15/12/2021
	"""

	def index_bam_file(input_bam):

		# index bam file
		try:
			pysam.check_index(input_bam)

		except AttributeError:
			sys.stderr.write(f"[{at()}] BAM index not found. Indexing file...\n")
			#calculate mean cov
			if input_bam[-3:] == "bam":
				bam = input_bam
				sys.stderr.write(f"[{at()}] Input alignment is BAM format: indexing it...\n")
				pysam.index(bam)

			elif input_bam[-3:] == "sam":
				sys.stderr.write(f"[{at()}] Input alignment is SAM format: converting to BAM and indexing it...\n")
				output_bam = f"{output_dir}/process/{sample}.{handle}.bam"
				pysam.view("-h", "-b", "-o", output_bam, input_bam, catch_stdout=False)
				bam = output_bam
				pysam.index(bam)

			else:
				sys.stderr.write(f"[{at()}] ERROR: {input_bam} does not end in \"sam\" or \"bam\". Can't determine the file type")

		return bam


	def get_global_coverage(bam):

		"""
		v1
		Author: Matteo Schiavinato
		Note: Get global mean coverage per position of a reference genome
		Based on the number of positions that are covered
		And the number of reads covering them
		Last update: 15/12/2021
		"""

		Covs = {}
		for chrom in list(bam.references):
			cov_count = bam.count_coverage(chrom, quality_threshold=0, read_callback="all")
			cov_count = [ [i for i in cov_count[k] if int(i) > 0] for k in (0,1,2,3) ]
			tot_pos = sum([ len(x) for x in cov_count ])
			tot_cov = sum([ sum(x) for x in cov_count ])
			try:
				cov = float(tot_cov) / float(tot_pos)
			except ZeroDivisionError:
				cov = float(0)
			Covs[chrom] = cov

		Covs  = { chrom : Covs[chrom] for chrom in Covs.keys() if Covs[chrom] > 0 }
		global_cov = sum(Covs.values()) / len(Covs.values())
		return (global_cov, Covs)


	def check_region_coverage(bed, bam, covfr, meancov, cmax):

		"""
		v1
		Author: Leszek Pryszcz
		Last update: 2014
		###
		v2
		Author: Matteo Schiavinato
		Note: Updated to python3
		Updated to new version of pysam
		Now using coverage per position instead of raw read count
		Last Update: 15/12/2021
		"""

		# unload bed coordinate
		chrom, start, end, length = bed.rstrip("\n\b\r").split('\t')
		start, end = int(start), int(end)

		# get coverage per position
		# count_coverage() returns a tuple with 4 tuples, each one containing the coverage per position
		# of a certain base (A,C,T,G) --> (0,1,2,3) in the command below
		cov_count = bam.count_coverage(chrom, start, end, quality_threshold=0, read_callback="all")
		cov_count = [ [i for i in cov_count[k] if int(i) > 0] for k in (0,1,2,3) ]
		tot_pos = sum([ len(x) for x in cov_count ])
		tot_cov = sum([ sum(x) for x in cov_count ])
		cov = float(tot_cov) / float(tot_pos)
		cov_ratio = cov / meancov

		# check if within boundaries
		if cov < float(covfr) * meancov or cov > float(cmax) * meancov:
			cov = round(cov, 2)
			bed_line = f"{chrom}\t{start}\t{end}\t{round(cov_ratio, 2)}\t{length}\n"
			return bed_line
		else:
			return False


	# index and open bam file
	indexed_bam = index_bam_file(input_bam)
	bam = pysam.AlignmentFile(indexed_bam, "rb")

	# get mean global coverage
	(meancov, Covs) = get_global_coverage(bam)
	sys.stderr.write(f"[{at()}] Mean coverage: {round(meancov, 2)}x\n")

	# create file with chromosomal coverages
	OUTPUT = open(f"{output_dir}/process/{sample}.{handle}.chrom_coverage.tsv", "w")
	for chrom in Covs:
		OUTPUT.write(f"{chrom}\t{Covs[chrom]}\n")
	OUTPUT.close()

	# open bed file
	bedstream = open(bed_file, "r")

	#select windows
	Out = [ check_region_coverage(bed, bam, covfr, meancov, cmax) for bed in bedstream ]
	removed = len([ i for i in Out if i == False ])
	Out = [ i for i in Out if i != False ]
	kept = len(Out)

	sys.stderr.write(f"[{at()}] Kept {kept} candidate LOH regions based on their coverage ({round((kept / (kept + removed))*100, 2)} %) out of {kept+removed} regions\n")

	return Out


def process_vcf_file(	in_vcf, output_dir, genome_file, bam, sample, handle,
						min_af, max_af, min_het_snps, snp_distance,
						min_frac_cov, max_frac_cov, min_size):

	"""
	v1
	Series of operations that a VCF file goes through to extract LOH blocks
	Last edit: 15/12/2021
	"""

	# Extract heterozygous snps from raw VCF file
	sys.stderr.write(f"[{at()}] Extracting heterozygous SNPs from VCF file...\n")
	sys.stderr.write(f"[{at()}] Considering only SNPs with {min_af} <= AF <= {max_af}...\n")
	het_snps_vcf = f"{output_dir}/{sample}.{handle}.het_snps.vcf"

	(result, het_snps_vcf) = extract_heterozygous_snps(	in_vcf,
														min_af,
														max_af,
														output_dir,
														sample,
														het_snps_vcf)

	# -------------------------------------------
	# Get heterozygous blocks with bedtools merge
	# merge bed file
	sys.stderr.write(f"[{at()}] Defining heterozygous blocks for {sample}...\n")
	merged_bed_out = f"{output_dir}/process/{sample}.{handle}.d{snp_distance}bp_provisory.bed"

	result = count_snps_in_intervals(	sample,
								 		output_dir,
								 		het_snps_vcf,
								 		merged_bed_out,
								 		snp_distance)

	# ---------------------------------------------
	# run filtering function on the merged bed file
	sys.stderr.write(f"[{at()}] Keeping only blocks with > {args.min_het_snps} SNP...\n")
	filt_bed = f"{output_dir}/process/{sample}.{handle}.d{snp_distance}bp.bed"

	(result, skipped) = filter_by_snp_density(	merged_bed_out,
												filt_bed,
												args.min_het_snps)

	sys.stderr.write(f"[{at()}] {skipped} regions were removed due to insufficient SNP count\n")

	# ------------------------------------------------------
	# Obtaining complementary regions to heterozygous blocks
	# get complementary bed file
	sys.stderr.write(f"[{at()}] Getting homozygous regions (i.e. candidate LOH blocks)...\n")
	complement_bed = f"{output_dir}/process/{sample}.{handle}.d{snp_distance}bp.complement.bed"

	result = get_complementary_regions(	filt_bed,
										complement_bed,
										genome_file)

	# -------------------------------------------
	# Get heterozygous blocks with bedtools merge
	# merge bed file
	sys.stderr.write(f"[{at()}] Combining homoyzgous regions closer than: {args.block_dist} bp...\n")
	merged_homo_bed_out = f"{output_dir}/process/{sample}.{handle}.d{snp_distance}bp.complement.merged.bed"

	result = merge_bed_homo_intervals(	sample,
								 		output_dir,
								 		complement_bed,
								 		merged_homo_bed_out,
										args.block_dist)

	# --------------------------------------------------
	# Filter the complement to get the homozygous blocks
	sys.stderr.write(f"[{at()}] Removing candidate blocks shorter than: {min_size} bp...\n")
	filt_compl_bed = f"{output_dir}/process/{sample}.{handle}.homo.d{snp_distance}bp.bed"

	(result, removed) = filter_by_length(	complement_bed,
							  				filt_compl_bed,
							  				min_size)

	sys.stderr.write(f"[{at()}] {removed} candidate blocks were removed due to insufficient length\n")

	# -----------------------------------------
	# filter by real coverage from the BAM file
	# use the filtered bed file as input for next step
	Final_bed_lines = filter_by_coverage(	bam,
											filt_compl_bed,
											min_frac_cov,
											max_frac_cov,
											output_dir,
											sample,
											handle)

	final_bed_file = f"{output_dir}/process/{sample}.{handle}.LOH_blocks.bed"

	# write final bed file lines to an actual bed file output
	OUT = open(final_bed_file, "w")
	for line in Final_bed_lines:
		OUT.write(line)
	OUT.close()

	return (True, final_bed_file)


def remove_intersection_intervals(Out, unique_t1000_bed):

	"""
	v1
	Intersect t0 and t1000 VCF files
	Keep only LOH events found in t1000 but not in t0
	Last edit: 13/12/2021
	"""

	t1000_bed = Out["exp"]
	t0_bed = Out["t0"]

	bt_A = BedTool(t1000_bed)
	bt_B = BedTool(t0_bed)
	bt_inter = bt_A.intersect(bt_B, v=True)
	bt_inter.saveas(unique_t1000_bed)

	return (True, unique_t1000_bed)


def keep_intersection_intervals(Out, unique_t1000_bed):

	"""
	v1
	Intersect t0 and t1000 VCF files
	Keep only LOH events found in both t1000 and t0
	Last edit: 13/12/2021
	"""

	t1000_bed = Out["exp"]
	t0_bed = Out["t0"]

	bt_A = BedTool(t1000_bed)
	bt_B = BedTool(t0_bed)
	bt_inter = bt_A.intersect(bt_B, u=True)
	bt_inter.saveas(unique_t1000_bed)

	return (True, unique_t1000_bed)


def main():

	"""
	Function carrying on the main operations
	"""

	# --------------------------------------
	# if required print script info and quit
	if args.print_info:
		print(script_info)
		sys.exit(0)

	# ---------------------------------------
	# create output directory if not existing
	sys.stderr.write(f"[{at()}] Preparing workspace...\n")
	(result, output_dir) = organize_workspace(	args.output_dir,
												args.min_frac_cov,
												args.max_frac_cov,
												args.bam)

	# iterate over all provided Vcfs
	if args.t0_vcf:
		Vcfs = { args.vcf : "exp", args.t0_vcf : "t0" }
		Bams = { args.vcf : args.bam, args.t0_vcf : args.t0_bam }
	else:
		Vcfs = { args.vcf : "exp"}
		Bams = { args.vcf : args.bam}

	Out = { handle : "" for handle in Vcfs.keys() }

	for in_vcf in Vcfs.keys():

		sys.stderr.write(f"[{at()}] Working on: {in_vcf}\n")

		handle = Vcfs[in_vcf]
		bam = Bams[in_vcf]

		(result, out_vcf) = process_vcf_file(
							in_vcf, args.output_dir, args.genome_file,
							bam, args.sample, handle,
							args.min_af, args.max_af, args.min_het_snps, args.snp_distance,
							args.min_frac_cov, args.max_frac_cov, args.min_size)

		Out[handle] = out_vcf

	# if t0 is provided, remove LOH blocks found in t0 from those found in t1000
	# these are the background noise of pre-existing LOH blocks
	# that are found at both t0 and t1000
	# we're interested only in those found at t1000 which have evolved after t0
	unique_t1000_bed = f"{output_dir}/{args.sample}.LOH_blocks.filt.bed"

	if args.t0_vcf:

		sys.stderr.write(f"[{at()}] Handling blocks found in \"t0\" with mode: \"{args.t0_filter_type}\"\n")

		if args.t0_filter_type == "remove":
			(result, output_loh_bed) = remove_intersection_intervals(Out, unique_t1000_bed)

		elif args.t0_filter_type == "keep":
			(result, output_loh_bed) = keep_intersection_intervals(Out, unique_t1000_bed)

		else:
			sys.stderr.write(f"[{at()}] ERROR: the --t0-filter-type option had a strange specification: \"{args.t0_filter_type}\". Only \"keep\" or \"remove\" can be specified.\n")
			sys.exit()

	# if no t0, just copy the previous output file
	else:
		output_loh_bed = Out["exp"]
		cmd_copy = f"cp {output_loh_bed} {unique_t1000_bed}"
		subprocess.run([cmd_copy], check=True, shell=True)

	sys.stderr.write(f"[{at()}] Done!\n")

# run the script
if __name__=='__main__':
  main()

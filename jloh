#!/usr/bin/env python3

# version and info
script_info = """
###
Author: Matteo Schiavinato
Based on the work of: Leszek Pryszcz (2014) and Veronica Mixao (2018)
Last change: 18/02/2022
###
"""

# import modules
import argparse as ap
import os
from os.path import getsize
import sys
import pysam
import subprocess
from time import asctime as at
import time
import pybedtools
from pybedtools import BedTool
from math import log10
import multiprocessing

# help section
if len(sys.argv) == 1:
	sys.argv.append("--help")

if (sys.argv[1] in ["--help", "-h", "-help", "help", "getopt", "usage"]):
	sys.stderr.write("""

J LOH
Still the one from the block
-
Extact LOH blocks from SNPs, coverage, and a reference genome
-
v 0.11.0
--------------------------------------------------------------------------------
by Matteo Schiavinato
largely based on:
Pryszcz et al., 2014	https://doi.org/10.1093/gbe/evu082
Mixao et al., 2019		https://doi.org/10.3389/fgene.2019.00383
--------------------------------------------------------------------------------

Usage:
./jloh --vcf <VCF> --genome-file <GENOME_FILE> --bam <BAM> [options]

[I/O/E]
--vcf               Input VCF file                                              [!]
--genome-file       File with chromosome lengths (chromosome <TAB> size)        [!]
--bam               BAM file used to call the --vcf variants                    [!]
--sample            Sample name / Strain name for output files                  [loh_blocks]
--output-dir        Output directory                                            [loh_blocks_out]
--print-info        Show authors and edits with dates                           [off]
--threads           Number of parallel computing threads                        [4]

[parameters]
--filter-mode       "pass" to keep only PASS variants, "all" to keep everything [all]
--min-snps-kbp      Min. num. SNPs in hetero-/homo-zygous region                [1]
--snp-distance      Max. distance (bp) between SNPs for blocks definition       [100]
--block-dist        Combine LOH blocks into one if closer than this distance    [100]
--min-size          Min. LOH block size                                         [100]
--min-af            Min. allele frequency to consider a variant heterozygous    [0.3]
--max-af            Max. allele frequency to consider a variant heterozygous    [0.7]
--min-frac-cov      Min. fraction of LOH block that has to be covered by reads  [0.5]
--hemi              Frac. of the mean coverage under which LOH is hemizygous    [0.75]
--overhang          Kb up/downstream of the LOH block for coverage comparison   [5000]

[pre-existing variation]
--no-alleles        Don't use homozygous SNPs to assign LOH blocks to REF/ALT   [off]
--t0-vcf            VCF with variants to ignore from --vcf                      [off]
--t0-bam            BAM file used to call the --t0-vcf variants                 [off]
--t0-filter-type    What to do with t0 LOH events? "keep" or "remove"           [remove]


""")
	sys.exit(0)

# argument parser
p = ap.ArgumentParser(description="")
p.add_argument("--vcf", type=str)
p.add_argument("--bam", type=str)
p.add_argument("--genome-file")
p.add_argument("--sample", default="loh_blocks")
p.add_argument("--filter-mode", choices=["all","pass"], default="all")
p.add_argument("--keep-secondary", action="store_true", default=False)
p.add_argument("--no-alleles", action="store_true", default=False)
p.add_argument("--output-dir", default="loh_blocks_out")
p.add_argument("--t0-vcf", type=str, default=None)
p.add_argument("--t0-bam", type=str, default=None)
p.add_argument("--t0-filter-type", choices=["keep", "remove"], default="remove")
p.add_argument("--min-snps-kbp", type=float, default=1)
p.add_argument("--snp-distance", default=100, type=int)
p.add_argument("--block-dist", default=100, type=int)
p.add_argument("--min-size", default=100, type=int)
p.add_argument("--min-af", default=0.3, type=float)
p.add_argument("--max-af", default=0.7, type=float)
p.add_argument("--min-frac-cov", default=0.5, type=float)
p.add_argument("--hemi", default=0.75, type=float)
p.add_argument("--overhang", default=5000, type=int)
p.add_argument("--threads", default=4, type=int)
p.add_argument("--print-info", action="store_true")
args = p.parse_args()


# functions
def dump_queue(q):

	out = []
	# while q.qsize() > 0:
	while not q.empty():
		x = q.get()
		out.append(x)

	return out

def organize_workspace(args):

	"""
	Last update: 18/02/2022
	Creation of folders and workspace where the script has to operate
	Verify presence of minimal set of files to work with
	"""

	if os.path.exists(args.output_dir) == False:
		os.makedirs(args.output_dir)
		sys.stderr.write(f"[{at()}] Created output directory {args.output_dir}\n")

	return (True, args.output_dir)


def hetero_and_homo_snps(args, vcf, handle):

	"""
	Last update: 16/02/2022
	"""
	het_snps_vcf = f"{args.output_dir}/{args.sample}.{handle}.het_snps.vcf"
	homo_snps_vcf = f"{args.output_dir}/{args.sample}.{handle}.homo_snps.vcf"

	def keep_heterozygous(args, Vcf_lines):

		"""
		Last update: 16/12/2021
		Filtering variants based on three criteria:
		- are they SNPs?
		- are they heterozygous?
		- if selected: do they PASS?
		"""

		kept, removed, homozygous, New_lines, Homo_lines = 0,0,0,[],[]

		for line in Vcf_lines:

			# split by field
			lst = line.rstrip("\b\r\n").split("\t")

			if ((args.filter_mode == "pass") and (lst[6] == "PASS")) or \
			(args.filter_mode == "all"):

				# read values
				annotations = lst[8].split(":")
				values = lst[9].split(":")
				dict = { annotations[i]:values[i] for i in range(0, len(annotations)) }

				# if it's a single heterozygous SNP
				if ((len(lst[3]) == len(lst[4]) == 1) and (dict["GT"]=="0/1")):

					# does AF fit the criteria?
					if (args.min_af <= float(dict["AF"]) <= args.max_af):

						# 4. write out lines that have fitting values
						New_lines.append(line)
						kept += 1

					# remove those without the frequency in range
					else:
						removed += 1

				# if homozygous: keep for later assignment to blocks
				elif ((len(lst[3]) == len(lst[4]) == 1) and (dict["GT"]=="0/0") \
				or (len(lst[3]) == len(lst[4]) == 1) and (dict["GT"]=="1/1")):

					Homo_lines.append(line)
					homozygous += 1

				# consider multiallelic sites
				# only if all alleles are SNPs
				# and if all alleles are not stars (spanning deletions)
				elif ((len(lst[3]) == 1) and (len(lst[4].split(",")) > 1)
				and (all([ len(x)==1 for x in lst[4].split(",") ])) \
				and (all([ x!="*" for x in lst[4].split(",") ]))):

					# this means that there are two annotations for AF
					# and that both have to be checked
					# so it is a variation on the previous block
					# which could be rendered into a function
					# assuming that there are > 1 AF annotation
					# splitting the field based on the comma
					# and converting to float the content
					AFs = [ float(x) for x in dict["AF"].split(",") ]

					# all it takes is one of the variants to be heterozygous
					# for the locus to be conserved
					# where het = AF comprised between --min-af and --max-af
					Conditions = [ float(args.min_af) <= af <= float(args.max_af) for af in AFs]
					if any(Conditions) == True:

						# 4. write out lines that have fitting values
						New_lines.append(line)
						kept += 1

					# remove those without any heterozygous snp
					# among those available
					else:
						removed += 1

				# remove homozygous SNPs
				# and indels
				else:
					removed += 1
			# exclude header lines starting with "#"
			else:
				removed += 1

		return (New_lines, Homo_lines, kept, removed, homozygous)

	# ------------------------------------

	# read SNPs
	INPUT = open(vcf, "r")
	Lines = [ line for line in INPUT ]
	INPUT.close()
	Header = [ line for line in Lines if line[0] == "#" ]
	Vcf_lines = [ line for line in Lines if line[0] != "#" ]

	# keep only heterozygous
	# if args.filter_mode == "pass", filter snps keeping only PASS
	(Vcf_lines, Homo_lines, kept, removed, homozygous) = keep_heterozygous(args, Vcf_lines)
	sys.stderr.write(f"[{at()}] Kept {kept} heterozygous SNPs ({round((kept / (kept + removed + homozygous))*100, 2)} %) out of {kept+removed+homozygous} VCF records\n")
	sys.stderr.write(f"[{at()}] Kept {homozygous} homozygous SNPs ({round((homozygous / (kept + removed + homozygous))*100, 2)} %) out of {kept+removed+homozygous} VCF records\n")
	sys.stderr.write(f"[{at()}] Discarded {removed} variant records due to their nature (indels, low/high AF)\n")

	# write filtered vcf file lines to an actual vcf file output
	OUT = open(het_snps_vcf, "w")
	for line in Header:
		OUT.write(line)
	for line in Vcf_lines:
		OUT.write(line)
	OUT.close()

	# write homozygous VCF file
	OUT = open(homo_snps_vcf, "w")
	for line in Header:
		OUT.write(line)
	for line in Homo_lines:
		OUT.write(line)
	OUT.close()

	return het_snps_vcf, homo_snps_vcf


def get_intervals_of_snps(snps_vcf, snp_distance):

	"""
	Last update: 22/12/2021
	Create BED regions based on the input VCF rows after their filtering
	These regions merge together SNPs as long as they are within --snp-distance
	For each, the number of variants contained in it are counted
	This function is used both with hetero and homozygous SNPs
	The output is: chr | start | end | count
	"""

	snps_vcf = BedTool(snps_vcf)
	bt_merged = snps_vcf.merge(d=snp_distance, c=1, o="count")

	return bt_merged


def filter_by_snp_density(bt, min_num_snps, discard):

	"""
	v1
	Author: Leszek Pryszcz
	Last update: 14/02/2014
	###
	v2
	Author: Matteo Schiavinato
	Last update: 18/02/2022
	"""

	if discard == False:
		out = []
		skipped = 0
		for row in bt:
			ref,start,end,count = str(row).rstrip("\n\r\b").split("\t")
			ref,start,end,count = str(ref),int(start),int(end),float(count)
			length_kb = (end-start) / 1000
			density = float(count) / float(length_kb)
			min_density = float(min_num_snps) / float(length_kb)
			if float(density) >= float(min_density):
				out.append(row)
			else:
				skipped += 1

		new_bt = BedTool(out)

	elif discard == True:
		out = []
		skipped = 0
		for row in bt:
			ref,start,end,count = str(row).rstrip("\n\r\b").split("\t")
			ref,start,end,count = str(ref),int(start),int(end),float(count)
			length_kb = (end-start) / 1000
			density = float(count) / float(length_kb)
			max_density = float(min_num_snps) / float(length_kb)
			if float(density) < float(max_density):
				out.append(row)
			else:
				skipped += 1

		new_bt = BedTool(out)

	return(new_bt, skipped)


def filter_by_length(bt, min_size):

	"""
	Last update: 21/12/2021
	Note: Keep only candidate LOH regions of a certain minimum size
	"""

	lst = [ str(row).rstrip("\b\r\n").split("\t") for row in bt ]
	lst = [ x for x in lst if int(x[2]) - int(x[1]) >= min_size ]
	new_bt = BedTool(lst)
	return new_bt


def get_snp_intervals(args, in_vcf, handle, invert):

	"""
	Last update: 18/02/2022
	"""

	# -------------------------------------------
	# Get blocks with SNPs with bedtools merge
	# merge bed file
	# then filter it by SNP density
	if invert == False:

		snps_vcf = BedTool(in_vcf)
		snp_blocks = get_intervals_of_snps(	snps_vcf, args.snp_distance)
		(snp_blocks, skipped) = filter_by_snp_density(snp_blocks, args.min_snps_kbp, False)
		snp_blocks = filter_by_length(snp_blocks, args.min_size)

	#
	elif invert == True:
		snps_vcf = BedTool(in_vcf)
		snp_blocks = get_intervals_of_snps(	snps_vcf, args.snp_distance)
		(snp_blocks, skipped) = filter_by_snp_density(snp_blocks, args.min_snps_kbp, True)
		snp_blocks = filter_by_length(snp_blocks, args.min_size)

	return snp_blocks



def remove_overlapping_regions(bt_A, bt_B, min_length):

	"""
	Last update: 16/02/2022
	"""

	# remove intersection between the two beds
	bt = bt_A.subtract(b=bt_B)
	# remove short ones
	lst = [str(x).rstrip("\b\r\n").split("\t") for x in bt]
	lst = [x for x in lst if int(x[2])-int(x[1]) >= min_length]
	# sort
	bt = bt.sort()
	# write to output
	return bt


def add_column(bt, annot):

	"""
	Last update: 21/12/2021
	Note: Add the "annot" value as extra column to a BED file
	"""

	x = [ str(row).rstrip("\b\r\n") + "\t" + str(annot) for row in bt ]
	new_bt = BedTool(x)
	return new_bt


def combine_beds(bt_A, bt_B):

	"""
	Last update: 21/12/2021
	Note: combine two bed files into one
	"""

	x = list(bt_A)
	y = list(bt_B)
	z = x + y
	bt = BedTool(z).sort()

	return bt


def parse_chromosomes(genome_file):

	"""
	Last update: 16/02/2022
	"""

	INPUT = open(genome_file)
	Chrom_lengths = [ line.rstrip("\n\r\b").split("\t") for line in INPUT ]
	Chrom_lengths = { x[0]:int(x[1]) for x in Chrom_lengths }
	return Chrom_lengths


def get_chromosomal_covered_regions(queue, lock, in_bam, chrom, chrom_len):

	"""
	Last update: 18/02/2022
	"""

	# read bam file and convert into bed file
	bt = BedTool(in_bam)
	bt = bt.bamtobed()

	# merge overlapping intervals
	bt = bt.merge()

	# convert to list
	bt = [ str(row) for row in bt ]

	# put in queue
	queue.put(bt)


def get_uncovered_regions(queue, lock, args, Chrom_lengths, in_bam):

	"""
	Last update: 16/02/2022
	"""

	# start pool
	pool = multiprocessing.Pool(processes=args.threads)

	# run function
	for chrom in Chrom_lengths.keys():
		pool.apply_async(get_chromosomal_covered_regions, args=(queue, lock, in_bam, chrom, Chrom_lengths[chrom]))

	# terminate pool
	pool.close()
	pool.join()

	# dump queue
	out = dump_queue(queue)

	# flatten the list
	cov_regions_bed = [x for sublst in out for x in sublst]

	# convert to bed tool
	bt = BedTool(cov_regions_bed).sort(faidx=args.genome_file)

	# invert interval
	bt_complement = bt.complement(g=args.genome_file)

	return bt_complement


def fix_coordinates_by_cov(Homo_blocks, bt_uncov_regions, min_length):

	"""
	Last update: 16/02/2022
	"""

	# intersect the two bed files (blocks and uncovered regions)
	# adjust coordinates
	bt_blocks = BedTool(Homo_blocks)
	bt_uncov = BedTool(bt_uncov_regions)
	bt_blocks_adj = bt_blocks.subtract(b=bt_uncov)

	# remove short ones
	adj_blocks = [ str(x).rstrip("\n\r\b").split("\t") for x in bt_blocks_adj ]
	adj_blocks = [ x for x in adj_blocks if int(x[2])-int(x[1]) >= min_length ]
	bt_blocks_adj = BedTool(adj_blocks)

	# sort
	bt_blocks_adj = bt_blocks_adj.sort()

	return bt_blocks_adj


def index_bam(input_bam):

	"""
	v1
	Last update: 15/02/2022
	"""

	if os.path.exists(f"{input_bam}.bai") == False:
		try:
			sys.stderr.write(f"[{at()}] Index not found. Indexing bam file... \n")
			pysam.index(input_bam)
			return True
		except:
			return False

	else:
		return True


def get_total_cov_pos(lock, input_bam, chrom, start, end):

	"""
	v1
	Last update: 16/02/2022
	"""

	if start < 0:
		start = 0

	# open reader
	# select reads within the region and crop at the end (truncate)
	bam = pysam.AlignmentFile(input_bam)
	pileup = bam.pileup(chrom, start, end, truncate=True)
	# get total covered positions using .n which is the number of reads in the "column"
	tot_cov_pos = len([i for i in pileup if int(i.n) > 0])

	return tot_cov_pos


def get_coverage(lock, input_bam, chrom, start, end):

	"""
	v1
	Last update: 16/02/2022
	"""

	# get total covered positions
	tot_cov_pos = get_total_cov_pos(lock, input_bam, chrom, start, end)

	try:
		bam = pysam.AlignmentFile(input_bam)
		pileup = bam.pileup(chrom, start, end, truncate=True)
		avg_cov = round(float(sum([int(i.n) for i in pileup if int(i.n) > 0])) / float(tot_cov_pos), 1)
	except ZeroDivisionError:
		avg_cov = round(float(0), 1)

	return avg_cov


def mp_chrom_coverage(queue, lock, input_bam, chrom, Chrom_lengths):

	"""
	v1
	Last update: 16/02/2022
	"""

	start = 0
	end = Chrom_lengths[chrom]

	# open reader
	# select reads within the region and crop at the end (truncate)
	bam = pysam.AlignmentFile(input_bam)
	pileup = bam.pileup(chrom, start, end, truncate=True)
	# get total covered positions using .n which is the number of reads in the "column"
	tot_cov_pos = len([i for i in pileup if int(i.n) > 0])

	try:
		# get average coverage of the region
		bam = pysam.AlignmentFile(input_bam)
		pileup = bam.pileup(chrom, start, end, truncate=True)
		avg_cov = round(float(sum([int(i.n) for i in pileup if int(i.n) > 0])) / float(tot_cov_pos), 1)
	except ZeroDivisionError:
		avg_cov = round(float(0), 1)

	queue.put((chrom, avg_cov))


def get_chrom_coverages(queue, lock, input_bam, output_dir, handle, Chrom_lengths, args):

	"""
	v1
	Author: Matteo Schiavinato
	Last change: 16/02/2022
	"""

	# initiate pool
	pool = multiprocessing.Pool(processes=args.threads)

	# run
	for chrom in Chrom_lengths.keys():
		pool.apply_async(mp_chrom_coverage, args=(queue, lock, input_bam, chrom, Chrom_lengths))

	# close and join pool so all processes must finish
	pool.close()
	pool.join()

	out = dump_queue(queue)
	Covs = { x[0]:x[1] for x in out }
	return Covs


def get_zygosity(lock, block_cov, input_bam, chrom, start, end, overhang, hemi_threshold, chrom_len):

	"""
	v1
	Last update: 16/02/2022
	"""

	# define up/down coordinates
	up_start = max(start-overhang, 0)
	up_end = start
	up_length = up_end - up_start
	down_start = end
	down_end = min(end+overhang, chrom_len)
	down_length = down_end - down_start

	# get cov ratio with 5 kb upstream
	# avoiding negative coordinates
	up_cov = get_coverage(lock, input_bam, chrom, up_start, start)
	# get cov ratio with 5 kb downstream
	# avoiding positions outside of the chrom max len in 0-based format
	down_cov = get_coverage(lock, input_bam, chrom, end, down_end)

	# compare with block coverage
	if up_length >= 0.9 * overhang:
		try:
			# up_ratio = min(block_cov / up_cov * 100, 100)
			up_ratio = block_cov / up_cov
		except ZeroDivisionError:
			up_ratio = 100
	else:
		up_ratio = None

	# down block
	if down_length >= 0.9 * overhang:
		try:
			# down_ratio = min(block_cov / down_cov * 100, 100)
			down_ratio = block_cov / down_cov
		except ZeroDivisionError:
			down_ratio = 100
	else:
		down_ratio = None

	# proceed with zygosity only if overhangs up/downstream are 90% of the chosen length at least
	# if not, write "NA"
	if all([x != None for x in [up_ratio, down_ratio]]):

		# if ratios are < --hemi,  block is hemizygous
		# if ratios are >= --hemi, block is homozygous
		if (up_ratio < hemi_threshold) and (down_ratio < hemi_threshold):
			zygosity = "hemi"
		elif (up_ratio >= hemi_threshold) and (down_ratio >= hemi_threshold):
			zygosity = "homo"
		else:
			zygosity = "NA"

	# if flanking regions are too small write "NA" directly (can't trust it)
	else:
		zygosity = "NA"

	return zygosity, up_ratio, down_ratio


def process_region_coverage(queue, lock, row, input_bam, min_frac_cov, hemi_threshold, Covs, overhang, Chrom_lengths):

	"""
	v1
	Author: Leszek Pryszcz
	Last update: 2014
	###
	v2
	Author: Matteo Schiavinato
	Last Update: 18/02/2022
	"""

	bed = str(row)

	# unload bed coordinate
	chrom, start, end, snps, block_type = bed.rstrip("\n\b\r").split('\t')
	chrom_len = Chrom_lengths[chrom]
	start, end = int(start), int(end)
	length = end-start
	chrom_meancov = float(Covs[chrom])

	# get total covererd positions
	tot_cov_pos = get_total_cov_pos(lock, input_bam, chrom, start, end)
	# get covered fraction by dividing the two values
	tot_pos = end-start
	cov_frac = round(float(tot_cov_pos) / float(tot_pos), 3)

	# debug:
	if cov_frac > 1.0:
		sys.stderr.write(f"\nERROR: the detected covered fraction ({cov_frac}) in {chrom}, {start}, {end} (0-based) is larger than the region size ({end-start} bp).\n")
		sys.stderr.write("Report this bug in a GitHub issue: it shouldn't happen!\n\n")
		sys.exit()

	if cov_frac >= min_frac_cov:

		# get coverage
		avg_cov = get_coverage(lock, input_bam, chrom, start, end)

		# get zygosity
		zygosity, up_ratio, down_ratio = get_zygosity(lock, avg_cov, input_bam, chrom, start, end, overhang, hemi_threshold, chrom_len)
		if up_ratio != None:
			up_ratio = round(up_ratio, 2)
		if down_ratio != None:
			down_ratio = round(down_ratio, 2)

		bed_line = f"{chrom}\t{start}\t{end}\t{avg_cov}x\t{cov_frac}\t{up_ratio}\t{down_ratio}\t{zygosity}\t{length}\t{block_type}\n"

	else:
		bed_line = None

	queue.put(bed_line)


def extract_chrom_lengths(chrom_lengths_file):

	"""
	v1
	Last update: 11/02/2022
	"""

	INPUT = open(chrom_lengths_file, "r")
	Lst = [ line.rstrip("\b\r\n").split("\t") for line in INPUT ]
	Chrom_lengths = { x[0]:int(x[1]) for x in Lst }
	INPUT.close()

	return Chrom_lengths


def assess_coverage(queue, lock, input_bam, bt, handle, args):

	"""
	v1
	Author: Leszek Pryszcz (2014), Veronica Mixao (2019)
	###
	v2
	Author: Matteo Schiavinato
	Last change: 16/02/2022
	"""

	# index bam file if needed
	sys.stderr.write(f"[{at()}] Checking bam file index... \n")
	res = index_bam(input_bam)
	if res == False:
		sys.exit("ERROR: The BAM file could not be indexed.\n\n")

	# get coverage of each chromosome independently
	sys.stderr.write(f"[{at()}] Getting mean coverage by chromosome... \n")
	Chrom_lengths = extract_chrom_lengths(args.genome_file)
	Covs = get_chrom_coverages(queue, lock, input_bam, args.output_dir, handle, Chrom_lengths, args)

	# write chromosome coverages to a file
	OUT = open(f"{args.output_dir}/{args.sample}.{handle}.chrom_coverage.tsv", "w")
	for chr in Covs:
		OUT.write(f"{chr}\t{Covs[chr]}\n")
	OUT.close()

	# calculate coverage in each block
	sys.stderr.write(f"[{at()}] Calculating coverage of LOH blocks... \n")
	# initiate pool again
	pool = multiprocessing.Pool(processes=args.threads)

	for row in bt:
		pool.apply_async(process_region_coverage, args=(queue, lock,
														row, input_bam, args.min_frac_cov,
														args.hemi, Covs,
														args.overhang, Chrom_lengths))

	pool.close()
	pool.join()

	Out = dump_queue(queue)

	Out = [ i for i in Out if i != None ]

	bt = BedTool(Out)
	return bt


def process_vcf_file(args, queue, lock, in_vcf, in_bam, handle):

	"""
	Last update: 18/02/2022
	"""

	# --------------------------------------------------------------------------
	# USE SNPS TO DEFINE HETEROZYGOUS INTERVALS AND HOMOZYGOUS "ALT" INTERVALS
	# select hetero and homo snps

	# mp
	pool = multiprocessing.Pool(processes=args.threads)

	sys.stderr.write(f"[{at()}] Extracting heterozygous and homozygous SNPs...\n")
	het_snps_vcf, homo_snps_vcf = hetero_and_homo_snps(args, in_vcf, handle)

	# get intervals with SNPs in bed format
	sys.stderr.write(f"[{at()}] Converting to SNP BED format...\n")

	Het_bed_blocks = pool.apply_async(get_snp_intervals, args=(args, het_snps_vcf, handle, False))

	if args.no_alleles == False:
		Homo_bed_blocks_ALT = pool.apply_async(get_snp_intervals, args=(args, homo_snps_vcf, handle, False))
		Homo_bed_blocks_REF = pool.apply_async(get_snp_intervals, args=(args, homo_snps_vcf, handle, True))
	elif args.no_alleles == True:
		Homo_bed_blocks = pool.apply_async(get_snp_intervals, args=(args, het_snps_vcf, handle, True))

	pool.close()
	pool.join()

	Het_bed_blocks = Het_bed_blocks.get()

	if args.no_alleles == False:
		Homo_bed_blocks_ALT = Homo_bed_blocks_ALT.get()
		Homo_bed_blocks_REF = Homo_bed_blocks_REF.get()

	elif args.no_alleles == True:
		Homo_bed_blocks = Homo_bed_blocks.get()

	# --------------------------------------------------------------------------
	# REMOVE OVERLAPS WITH HETEROZYGOUS BLOCKS
	# then filter by length within the same function
	# add the REF / ALT / NA annotation
	# combine into one file

	sys.stderr.write(f"[{at()}] Removing overlaps with heterozygous blocks...\n")

	if args.no_alleles == False:

		# mp
		pool = multiprocessing.Pool(processes=args.threads)

		Homo_bed_blocks_REF = pool.apply_async(remove_overlapping_regions, args=(Homo_bed_blocks_REF,
																				Het_bed_blocks,
																				args.min_size))

		Homo_bed_blocks_ALT = pool.apply_async(remove_overlapping_regions, args=(Homo_bed_blocks_ALT,
																				Het_bed_blocks,
																				args.min_size))


		pool.close()
		pool.join()

		Homo_bed_blocks_REF = Homo_bed_blocks_REF.get()
		Homo_bed_blocks_ALT = Homo_bed_blocks_ALT.get()

		Homo_REF = add_column(Homo_bed_blocks_REF, "REF")
		Homo_ALT = add_column(Homo_bed_blocks_ALT, "ALT")
		Homo_blocks = combine_beds(	Homo_REF, Homo_ALT)

	elif args.no_alleles == True:
		Homo_bed_blocks = remove_overlapping_regions(Homo_bed_blocks,
													Het_bed_blocks,
													args.min_size)

		Homo_blocks = add_column(Homo_bed_blocks, "NA")

	# --------------------------------------------------------------------------
	# REMOVE UNCOVERED REGIONS


	Chrom_lengths = parse_chromosomes(args.genome_file)

	sys.stderr.write(f"[{at()}] Extracting uncovered regions of the genome...\n")
	bt_cov_regions = get_uncovered_regions(queue, lock, args, Chrom_lengths, in_bam)

	sys.stderr.write(f"[{at()}] Adjusting block boundaries by coverage...\n")
	Homo_blocks_adj = fix_coordinates_by_cov(Homo_blocks, bt_cov_regions, args.min_size)

	# --------------------------------------------------------------------------
	# EXTRACT BLOCK COVERAGE

	sys.stderr.write(f"[{at()}] Extracting candidate block coverage...\n")
	LOH = assess_coverage(queue, lock, in_bam, Homo_blocks, handle, args)

	# ---
	return (LOH, homo_snps_vcf, het_snps_vcf)


def use_intersection_intervals(Out, filter_type):

	"""
	Last update: 21/12/2021
	Intersect t0 and t1000 VCF files
	"""

	# take heterozygous VCF files
	t1000_bed = Out["exp"][0]
	t0_bed = Out["t0"][0]

	# intersect files
	bt_A = BedTool(t1000_bed)
	bt_B = BedTool(t0_bed)

	if filter_type == "remove":
		bt_inter = bt_A.intersect(bt_B, v=True)
	elif filter_type == "keep":
		bt_inter = bt_A.intersect(bt_B, u=True)

	return bt_inter


def count_snps_in_bed_interval(bed, vcf):

	"""
	Last update: 21/12/2021
	Note: Count how many SNPs are found inside a BED interval
	by intersecting a BED file with a VCF file
	"""

	bt_inter = bed.intersect(vcf, c=True)
	return bt_inter


def main(args):

	"""
	Last update: 17/02/2022
	Note: Function carrying on the main operations
	"""

	# --------------------------------------------------------------------------
	# SET UP WORKSPACE

	# mp details
	queue = multiprocessing.Manager().Queue()
	lock = multiprocessing.Manager().Lock()

	# if required print script info and quit
	if args.print_info:
		print(script_info)
		sys.exit(0)

	# create output directory if not existing
	sys.stderr.write(f"[{at()}] Preparing workspace...\n")
	(result, output_dir) = organize_workspace(args)

	# --------------------------------------------------------------------------
	# PROCESS EACH VCF FILE

	# iterate over all provided Vcfs and Bams
	if args.t0_vcf:
		Vcfs = { args.vcf : "exp", args.t0_vcf : "t0" }
		Bams = { args.vcf : args.bam, args.t0_vcf : args.t0_bam }
	else:
		Vcfs = { args.vcf : "exp"}
		Bams = { args.vcf : args.bam}

	Out = { handle : "" for handle in Vcfs.keys() }

	for in_vcf in Vcfs.keys():

		sys.stderr.write(f"[{at()}] Working on: {in_vcf}\n")

		handle = Vcfs[in_vcf]
		in_bam = Bams[in_vcf]

		(LOH, homo_snps, hetero_snps) = process_vcf_file(args, queue, lock, in_vcf, in_bam, handle)

		Out[handle] = [LOH, homo_snps, hetero_snps]
		out = f"{output_dir}/{args.sample}.{handle}.LOH_candidates.tsv"
		LOH.saveas(out)

	# --------------------------------------------------------------------------
	# REMOVE T0 VARIATION IF PROVIDED

	# if t0 is provided, remove LOH blocks found in t0 from those found in t1000
	# these are the background noise of pre-existing LOH blocks
	# that are found at both t0 and t1000
	# we're interested only in those found at t1000 which have evolved after t0
	if args.t0_vcf:
		sys.stderr.write(f"[{at()}] Handling blocks found in \"t0\" with mode: \"{args.t0_filter_type}\"\n")
		LOH_filt = use_intersection_intervals(Out, args.t0_filter_type)
	else:
		LOH_filt = Out["exp"][0]

	# --------------------------------------------------------------------------
	# COUNT SNPS IN BLOCKS

	# count SNPs in selected blocks
	# counting heterozygous
	# and then counting homozygous
	bt_homo = BedTool(Out["exp"][1])
	bt_hetero = BedTool(Out["exp"][2])
	# count homozygous first
	LOH_w_snps = count_snps_in_bed_interval(LOH_filt, bt_homo)
	# then add extra column for heterozygous
	LOH_w_snps = count_snps_in_bed_interval(LOH_w_snps, bt_hetero)

	# --------------------------------------------------------------------------
	# WRITE TO OUTPUT

	# write to output
	# including a header
	out = f"{output_dir}/{args.sample}.LOH_blocks.tsv"
	OUTPUT = open(out, "w")
	OUTPUT.write("\t".join(["#Chrom", "Start", "End", "Cov", "Cov_frac", "Cov_ratio_up", "Cov_ratio_down", "Zygosity", "Length", "Allele", "Homo_snps", "Hetero_snps"]) + "\n")
	for row in LOH_w_snps:
		OUTPUT.write(str(row))
	OUTPUT.close()

	# create output bed file
	out = f"{output_dir}/{args.sample}.LOH_blocks.bed"
	OUTPUT = open(out, "w")
	for row in LOH_w_snps:
		row = list(row)[0:3]
		OUTPUT.write("\t".join(row) + "\n")
	OUTPUT.close()

	sys.stderr.write(f"[{at()}] Done!\n")

# run the script
if __name__=='__main__':
  main(args)
